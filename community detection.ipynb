{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a443f1-34f2-4fc8-95d8-fa92e59a36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "import numpy as np\n",
    "from community import community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de34f377-c3be-4d77-a2af-85f2beab5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('reddit_comments.csv',encoding='utf-8')\n",
    "posts = pd.read_csv('reddit_posts.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21543f48-6fff-48a1-9545-e0e8de2c0067",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Posts = posts[['author', 'cleaned_tokens']]\n",
    "Comments = comments[['author', 'cleaned_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defab270-e0e2-4150-b738-3cd6b7530797",
   "metadata": {},
   "outputs": [],
   "source": [
    "lPosts = pd.concat([Posts, Comments], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe8bf09-3ec8-47bd-9dca-c8f76e870528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#str → list\n",
    "def parse_tokens(tokens):\n",
    "    if isinstance(tokens, str):\n",
    "        try:\n",
    "            return ast.literal_eval(tokens)\n",
    "        except:\n",
    "            return []\n",
    "    return tokens\n",
    "#color\n",
    "def hex_to_rgb(hex_color):\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f35a43c8-5755-4a5c-b30c-62eae970012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lposts(combined_df, output_prefix=\"lPosts\", min_freq=5):\n",
    "    clean_df = combined_df[\n",
    "        combined_df['author'].notna() & \n",
    "        ~combined_df['author'].isin(['[deleted]', '[removed]'])\n",
    "    ].copy()\n",
    "    clean_df['cleaned_tokens'] = clean_df['cleaned_tokens'].apply(parse_tokens)\n",
    "    edges = []\n",
    "    for _, row in clean_df.iterrows():\n",
    "        if isinstance(row['cleaned_tokens'], list):\n",
    "            edges.extend([{'user': row['author'], 'keyword': token} \n",
    "                         for token in row['cleaned_tokens']])\n",
    "    \n",
    "    edges_df = pd.DataFrame(edges)\n",
    "    keyword_counts = edges_df['keyword'].value_counts()\n",
    "    edges_df = edges_df[edges_df['keyword'].isin(keyword_counts[keyword_counts >= min_freq].index)]\n",
    "    edges_df.to_csv(f\"{output_prefix}_user_keyword.csv\", index=False)\n",
    "    return edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c277d343-da99-41e8-8a60-3544e76a97b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Own_Teacher3433</td>\n",
       "      <td>ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Own_Teacher3433</td>\n",
       "      <td>land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Own_Teacher3433</td>\n",
       "      <td>moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ms_Photon</td>\n",
       "      <td>tell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ms_Photon</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107756</th>\n",
       "      <td>EnoughNoLibsSpam</td>\n",
       "      <td>utterly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107757</th>\n",
       "      <td>EnoughNoLibsSpam</td>\n",
       "      <td>irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107758</th>\n",
       "      <td>EnoughNoLibsSpam</td>\n",
       "      <td>point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107759</th>\n",
       "      <td>EnoughNoLibsSpam</td>\n",
       "      <td>earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107760</th>\n",
       "      <td>EnoughNoLibsSpam</td>\n",
       "      <td>moon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1068218 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user     keyword\n",
       "0         Own_Teacher3433        ever\n",
       "1         Own_Teacher3433        land\n",
       "2         Own_Teacher3433        moon\n",
       "3               Ms_Photon        tell\n",
       "4               Ms_Photon       think\n",
       "...                   ...         ...\n",
       "1107756  EnoughNoLibsSpam     utterly\n",
       "1107757  EnoughNoLibsSpam  irrelevant\n",
       "1107758  EnoughNoLibsSpam       point\n",
       "1107759  EnoughNoLibsSpam       earth\n",
       "1107760  EnoughNoLibsSpam        moon\n",
       "\n",
       "[1068218 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lposts(lPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dadec9f-69b8-4a6a-8ab5-aceb4de00550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_analysis(filename, gephi_name, target_communities=None):\n",
    "    edges = pd.read_csv(filename, usecols=['user', 'keyword'], dtype={'user': 'category', 'keyword': 'category'})\n",
    "    user_nodes = edges['user'].unique()\n",
    "    keyword_nodes = edges['keyword'].unique()\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(user_nodes, bipartite=0)\n",
    "    G.add_nodes_from(keyword_nodes, bipartite=1)\n",
    "    edge_tuples = list(zip(edges['user'], edges['keyword']))\n",
    "    G.add_edges_from(edge_tuples)\n",
    "\n",
    "    G_undirected = G.to_undirected(as_view=True)\n",
    "    partition = community_louvain.best_partition(G_undirected, resolution=1.0, random_state=42)\n",
    "    print(f\"Total number of communities: {len(set(partition.values()))}\")\n",
    "    degrees = dict(G.degree())\n",
    "    all_communities = set(partition.values())\n",
    "    all_leaders = {\n",
    "        comm: max((n for n in G.nodes if partition[n] == comm), key=lambda x: degrees[x])\n",
    "        for comm in all_communities\n",
    "    }\n",
    "    for comm in sorted(all_leaders):\n",
    "        leader = all_leaders[comm]\n",
    "        print(f\"Community {comm}: Leader = '{leader}', Degree = {degrees[leader]}\")\n",
    "    if target_communities is not None:\n",
    "        filtered_nodes = [n for n in G.nodes if partition[n] in target_communities]\n",
    "        G = G.subgraph(filtered_nodes).copy()\n",
    "        partition = {n: partition[n] for n in filtered_nodes}\n",
    "\n",
    "    communities = set(partition.values())\n",
    "    num_communities = len(communities)\n",
    "    cmap = plt.colormaps['tab20'].resampled(num_communities)\n",
    "    community_colors = {}\n",
    "\n",
    "    for i, comm in enumerate(sorted(communities)):\n",
    "        rgb = cmap(i)[:3]\n",
    "        hex_color = '#%02x%02x%02x' % tuple(int(255*x) for x in rgb)\n",
    "        community_colors[comm] = hex_color\n",
    "\n",
    "    node_colors = {node: community_colors[comm] for node, comm in partition.items()}\n",
    "    nx.set_node_attributes(G, node_colors, 'color')\n",
    "\n",
    "    for comm, hex_col in community_colors.items():\n",
    "        rgb = hex_to_rgb(hex_col)\n",
    "        print(f\"Community {comm}: RGB{rgb}\")\n",
    "\n",
    "    community_sizes = pd.Series(partition).value_counts().sort_index()\n",
    "    print(\"Community Sizes:\\n\", community_sizes)\n",
    "\n",
    "    degrees = dict(G.degree())\n",
    "    community_leaders = {comm: max((n for n in G.nodes if partition[n] == comm), key=lambda x: degrees[x])\n",
    "                         for comm in communities}\n",
    "    print(\"\\nCommunity Leaders:\\n\", community_leaders)\n",
    "\n",
    "    node_sizes = {node: 10 for node in G.nodes}\n",
    "    for leader in community_leaders.values():\n",
    "        if leader in node_sizes:\n",
    "            node_sizes[leader] = 20\n",
    "    nx.set_node_attributes(G, node_sizes, 'size')\n",
    "\n",
    "    pos = {}\n",
    "    offset = 10_00\n",
    "    for i, comm in enumerate(sorted(communities)):\n",
    "        nodes_in_comm = [n for n in G.nodes if partition[n] == comm]\n",
    "        subgraph = G.subgraph(nodes_in_comm)\n",
    "\n",
    "        if len(nodes_in_comm) <= 3:\n",
    "            sub_pos = nx.random_layout(subgraph)\n",
    "        else:\n",
    "            sub_pos = nx.spring_layout(subgraph, seed=42, k=0.2, iterations=50)\n",
    "\n",
    "        dx, dy = (i * offset, i * offset)\n",
    "        pos.update({node: (x + dx, y + dy) for node, (x, y) in sub_pos.items()})\n",
    "\n",
    "    nx.set_node_attributes(G, {k: float(v[0]) for k, v in pos.items()}, 'x')\n",
    "    nx.set_node_attributes(G, {k: float(v[1]) for k, v in pos.items()}, 'y')\n",
    "\n",
    "    nx.write_graphml(G, f\"{gephi_name}.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6796c29f-98f3-4afe-83dc-c27389475734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of communities: 20\n",
      "Community 0: Leader = 'moon', Degree = 3120\n",
      "Community 1: Leader = 'would', Degree = 1705\n",
      "Community 2: Leader = 'Kazeite', Degree = 3163\n",
      "Community 3: Leader = 'CuteBananaMuffin', Degree = 5925\n",
      "Community 4: Leader = 'jonathan_92', Degree = 523\n",
      "Community 5: Leader = 'Ok_Magician_1194', Degree = 262\n",
      "Community 6: Leader = 'Buckyhead', Degree = 361\n",
      "Community 7: Leader = 'iriebeatz', Degree = 9\n",
      "Community 8: Leader = 'rawkstaugh', Degree = 23\n",
      "Community 9: Leader = 'Occumsmachete', Degree = 12\n",
      "Community 10: Leader = 'Claytertot', Degree = 30\n",
      "Community 11: Leader = 'Vo_Sirisov', Degree = 591\n",
      "Community 12: Leader = 'remindmeyears', Degree = 6\n",
      "Community 13: Leader = 'duh', Degree = 12\n",
      "Community 14: Leader = 'backwards', Degree = 34\n",
      "Community 15: Leader = 'Gullible-Success-601', Degree = 9\n",
      "Community 16: Leader = 'An_American1776', Degree = 119\n",
      "Community 17: Leader = 'SgtSharki', Degree = 15\n",
      "Community 18: Leader = 'ohhh', Degree = 4\n",
      "Community 19: Leader = 'Anomalistic_Username', Degree = 10\n",
      "Community 0: RGB(31, 119, 180)\n",
      "Community 3: RGB(214, 39, 40)\n",
      "Community 4: RGB(247, 182, 210)\n",
      "Community 5: RGB(158, 218, 229)\n",
      "Community Sizes:\n",
      " 0    4061\n",
      "3    7535\n",
      "4     842\n",
      "5      33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Community Leaders:\n",
      " {0: 'moon', 3: 'CuteBananaMuffin', 4: 'always', 5: 'Ok_Magician_1194'}\n"
     ]
    }
   ],
   "source": [
    "community_analysis(\"lPosts_user_keyword.csv\", \"lPosts\", target_communities=[0,3,5,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97b14b-c3c5-40f5-a798-463d9dac9ad2",
   "metadata": {},
   "source": [
    "HITS arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00a7d324-4ee5-43cd-b5e2-2c6ea1b23926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_analysis(filename, top_n=10):\n",
    "    edges = pd.read_csv(filename)\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        edges, \n",
    "        source='user', \n",
    "        target='keyword', \n",
    "        create_using=nx.DiGraph()\n",
    "    )\n",
    "    hubs, authorities = nx.hits(G, max_iter=100, tol=1e-08, nstart=None, normalized=True)\n",
    "    top_hubs = sorted(hubs.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_authorities = sorted(authorities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    print(\"\\nThe most important hub (users who quote important keywords):\")\n",
    "    for user, score in top_hubs:\n",
    "        print(f\"{user}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\nThe most important authoritative keywords (keywords quoted by important users):\")\n",
    "    for keyword, score in top_authorities:\n",
    "        print(f\"{keyword}: {score:.4f}\")\n",
    "    \n",
    "    return hubs, authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566cf97c-4212-4d1d-b764-26f7b85aed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The most important hub (users who quote important keywords):\n",
      "CuteBananaMuffin: 0.0042\n",
      "Kazeite: 0.0034\n",
      "EnoughNoLibsSpam: 0.0031\n",
      "TreyinHada: 0.0025\n",
      "SgtBrutalisk: 0.0024\n",
      "ConspiracyCornerNews: 0.0024\n",
      "CatEyes420: 0.0024\n",
      "trevorj414: 0.0023\n",
      "qwertycoder: 0.0023\n",
      "nickhintonn333: 0.0022\n",
      "\n",
      "The most important authoritative keywords (keywords quoted by important users):\n",
      "moon: 0.0024\n",
      "would: 0.0020\n",
      "like: 0.0019\n",
      "one: 0.0019\n",
      "people: 0.0019\n",
      "time: 0.0017\n",
      "landing: 0.0017\n",
      "think: 0.0017\n",
      "know: 0.0017\n",
      "even: 0.0016\n"
     ]
    }
   ],
   "source": [
    "hubs, authorities = hits_analysis(\"lPosts_user_keyword.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96adf306-e6cd-4c7b-85ba-437003f5de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_hits_analysis(filename, gephi_name, target_communities=None):\n",
    "    edges = pd.read_csv(filename)\n",
    "    G = nx.from_pandas_edgelist(edges, source='user', target='keyword', create_using=nx.DiGraph)\n",
    "    G_undirected = G.to_undirected()\n",
    "    partition = community_louvain.best_partition(G_undirected, resolution=1.0)\n",
    "    communities = set(partition.values())\n",
    "    if target_communities is not None:\n",
    "        communities = [c for c in communities if c in target_communities]\n",
    "    \n",
    "    for comm in sorted(communities):\n",
    "        print(f\"\\n=== commmunity {comm} HITS analysis ===\")\n",
    "        nodes_in_comm = [n for n in G.nodes if partition[n] == comm]\n",
    "        subgraph = G.subgraph(nodes_in_comm)\n",
    "        \n",
    "        if len(nodes_in_comm) > 10: \n",
    "            try:\n",
    "                hubs, authorities = nx.hits(subgraph)\n",
    "                top_hubs = sorted(hubs.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                top_auth = sorted(authorities.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                \n",
    "                print(\"\\nThe most important hub:\")\n",
    "                for user, score in top_hubs:\n",
    "                    print(f\"  {user}: {score:.4f}\")\n",
    "                \n",
    "                print(\"\\nThe most important keywords:\")\n",
    "                for keyword, score in top_auth:\n",
    "                    print(f\"  {keyword}: {score:.4f}\")\n",
    "                    \n",
    "            except nx.PowerIterationFailedConvergence:\n",
    "                print(\"HITS algorithm failed to converge with this community\")\n",
    "        else:\n",
    "            #HITS analysis those with enough node and edge community, if it has lots of public edges it will be small\n",
    "            print(\"The community is too small for HITS. Showing top degree nodes instead:\")\n",
    "            degrees = dict(subgraph.degree())\n",
    "            top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            for node, deg in top_nodes:\n",
    "                print(f\"  {node}: Degree = {deg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a67e509-161c-4b99-9390-d65f1bed14b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== commmunity 0 HITS analysis ===\n",
      "The community is too small for HITS. Showing top degree nodes instead:\n",
      "  collecting: Degree = 2\n",
      "  senatormerkin: Degree = 1\n",
      "  Jolcski: Degree = 1\n",
      "\n",
      "=== commmunity 3 HITS analysis ===\n",
      "\n",
      "The most important hub:\n",
      "  Kazeite: 0.0251\n",
      "  EnoughNoLibsSpam: 0.0199\n",
      "  canadian1987: 0.0125\n",
      "  4544BeersOnTheWall: 0.0121\n",
      "  CarbonSlayer72: 0.0121\n",
      "\n",
      "The most important keywords:\n",
      "  lunar: 0.0033\n",
      "  around: 0.0032\n",
      "  surface: 0.0031\n",
      "  orbit: 0.0029\n",
      "  without: 0.0028\n",
      "\n",
      "=== commmunity 4 HITS analysis ===\n",
      "\n",
      "The most important hub:\n",
      "  einzelkind: 0.3659\n",
      "  layomao: 0.3299\n",
      "  Squirrelboy85: 0.0705\n",
      "  ILoveBeerAndFishing: 0.0607\n",
      "  MycelialArchetype: 0.0581\n",
      "\n",
      "The most important keywords:\n",
      "  der: 0.0999\n",
      "  ich: 0.0933\n",
      "  auch: 0.0933\n",
      "  ist: 0.0933\n",
      "  den: 0.0933\n",
      "\n",
      "=== commmunity 5 HITS analysis ===\n",
      "The community is too small for HITS. Showing top degree nodes instead:\n",
      "  owen: Degree = 3\n",
      "  Level_Hovercraft_825: Degree = 3\n",
      "  No-Win-1137: Degree = 3\n"
     ]
    }
   ],
   "source": [
    "community_hits_analysis(\"lPosts_user_keyword.csv\", \"lPosts\", target_communities=[0,3,5,4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
